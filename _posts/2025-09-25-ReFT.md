---
layout: single
title: "ReFT: Reasoning with REinforced Fine-Tuning 리뷰"
categories: paper
tag: [강화학습, NLP]
author_profile: false
sidebar:
    nav: "counts"
toc: true
toc_sticky: true
toc_label: Table of Contents
use_math: true
---

# Background

보통 LLM의 추론 능력을 향상시키기 위해서 CoT annotation data로 학습을 한다. 이때 CoT annotation이란 단순히 질문에 대한 정답만 포함하는 것이 아니라, 정답에 도달하기까지의 논리적 사고 흐름을 함께 제공하는 데이터이다. 보통 질문에 대한 사고 흐름을 잘 맞추도록 이를 구성하는 token들을 잘 예측하도록 supervised learning으로 학습한다. 하지만 이 방법의 문제점은 질문에 대해 하나의 풀이만 학습한다는 것이다. 결국, LLM은 다른 풀이를 explore하지 못하게 되어 generalization이 떨어지게 된다.   
   

# ReFT 방법론

ReFT 논문은 이 문제를 해결하기 위해 다음 방법을 따른다. ReFT는 두 단계를 거쳐 학습한다.   

### 1. Warm up 단계   
(질문 x, 풀이 CoT e)로 구성된 dataset에 대해 Supervised Finetuning하여 일정 수준의 문제 해결 능력을 갖추는 단계이다. CoT e에 존재하는 token들을 next token prediction action의 sequence로 여겨서 policy $\pi_\theta$ 가 풀이를 잘 생성하도록 한다. 마지막 action token `<eos>`은 생성을 끝내도록 signal한다.   
CoT e = [ $a_1, a_2, ..., a_L$ = `<eos>`]. 이때 L은 생성할 수 있는 CoT의 maximum length이다. 즉, 매 timestep마다 policy $\pi_\theta(\cdot \mid s_t)$ 로부터 vocabulary의 임의의 token인 $a_t$을 sample한다고 보면 되고, $s_t$은 질문과 지금까지 생성된 token들의 concatenation이라고 보면 된다.   
![joowan1108]({{site.url}}/images/papers/ReFT/s_t.png) 

이 과정의 loss function은 ![joowan1108]({{site.url}}/images/papers/ReFT/SFT.png)

### 2. Reinforcement Learning 단계
이 단계에서는 강화학습을 통해 policy가 (질문 x, 답 y) data 로 online self learning을 한다. 
Self learning이란 model이 생성한 sample로 또 학습을 하는 과정이고 online이란 data가 하나씩 들어올 때마다 바로 model을 update하는 방식이다.   
자세하게 말하자면, 이 과정에서 PPO with clipped objective algorithm을 통해 policy를 update한다.

![joowan1108]({{site.url}}/images/papers/ReFT/policy_objective.png)

Policy를 PPO로 update하기 위해 우선 reward function $r(s_t, a_t, s_{t+1})$을 정의해야 한다. 
#### 2.1 Reward Function
 
  이 reward function은 $s_t$에서 $a_t$을 함으로써 얻는 즉각적 reward이다. 논문에서 $a_t$로 terminal state에 도달하지 못했다면 0, *EXTRACT*($s_{t+1}$) = y라면 1의 reward을 준다. Dataset 중에 답이 모두 숫자로 이루어진 dataset에 대해서는 답이 일치하지 않지만 *EXTRACT*될 수 있고 숫자로 되어있다면 0.1의 reward을 준다.
  ![joowan1108]({{site.url}}/images/papers/ReFT/reward_function.png)
  0.1과 같은 partial reward는 sparse한 reward로 학습을 할 때 생기는 부정적 effect를 줄여준다고 한다.

  최종적으로 $s_t$에서 $a_t$를 취함으로써 얻는 총 즉각적 reward를 논문에서 다음과 같이 정의한다.
  ![joowan1108]({{site.url}}/images/papers/ReFT/r_total.png)
  KL divergence를 통해 과도한 policy 변화가 일어날 때 reward를 감소하도록 설정했다.



#### 2.2 Value Function
  PPO은 지금 policy가 얼만큼 잘하고 있는지를 추정해서 그 값을 증가시키는 action은 자주 sample 되도록, 그 값을 감소시키는 action은 덜 sample 되도록 policy를 update한다. 잘한다의 기준은 policy가 sample하는 action $a_t$ 이 가져다주는 보상, 즉 advantage가 높은지에 따라 결정된다. 하지만 $a_t$가 미래에 가져다줄 모든 reward은 계산하기 어렵기 때문에 이 값을 estimate 할 수 있는 함수를 구해야 한다. 이런 함수를 value function이라고 한다. 
  ReFT 논문에서 warm up이 끝난 $\pi_\theta$의 마지막 hidden states 위에 linear value head을 추가하여 value function을 구성한다. 이 function은 state $s_t$에서부터 미래에 얻을 수 있는 총 보상의 기댓값을 계산한다.


#### 2.3 Advantage 계산
  PPO는 advantage를 높이는 $a_t$가 자주 sample되게 하는 방향으로 policy를 update한다고 했다. Advantage를 계산하는 방법에는 여러가지가 존재한다.
  
  1. Q-Learning: $A_t = Q_t(s_t, a_t) - V(s_t)$   
     여기서 $Q_t(s_t, a_t)$ 는 state  $s_t$에서  $a_t$를 sample함으로써 얻는 기대 누적 보상이고  $V(s_t)$는 state $s_t$에서 얻을 수 있는 기대 누적 보상이다. 하지만 Q를 계산하기 위해서는 모든 state, action 쌍에 대해서 미래 보상 기댓값을 계산해야 하기 때문에 $a_t$를 골랐을 때 policy에게 즉각적인 피드백을 주지 못한다. 이 문제를 해결하기 위해 temporal difference  $\delta_t$를 사용한다.

  2. Temporal Difference: $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$
     여기서는 $r_t + \gamma V(s_{t+1})$ 가 $Q_t(s_t, a_t)$를 대체한다고 보면 된다. $r_t + \gamma V(s_{t+1})$에서 $r_t$은 timestep t에서 $a_t$를 취함으로써 얻는 즉각적인 보상이고 $\gamma V(s_{t+1})$은 미래 state $s_{t+1}$에서부터 얻을 수 있는 기대 누적 보상이다. 이 방법의 장점은 앞선 방법처럼 모든 state, action 쌍에 대해 보상 기댓값을 계산하지 않아도 되며 상태값에만 depend하기 때문에 $a_t$를 골랐을 때 policy에게 즉각적인 피드백을 부트스트래핑을 통해 줄 수 있다. 즉, episode 종료를 기다리지 않고 즉시 policy를 update 할 수 있다. 
     하지만 이 방법 또한 문제점이 존재한다. 가치 함수 추정값 $V(s_t)$가 잘못 추청된다면 그 잘못된 추정값이 다음 update에도 반영이 되어 오류가 연쇄적으로 확대되고 누적될 수 있다. 이 문제를 해결하기 위해 Generalized Advantage Estimate 방법이 고안되었다.
  
  3. Generalized Advantage Estimate: $\hat{A_t}^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{L-t} (\gamma \lambda)^l \, \delta_{t+l}$
     GAE는 




 과정을 더 자세하게 설명하면 x가 주어졌을 때 Policy model $\pi_\theta$ 는 x를 푸는 CoT data $\hat{e}$ 를 일정 개수만큼 생성한다. 
모델이 CoT data에 대해 SFT를 warm up 단계에서 이미 하였기에 $\hat{e}$ 은 "풀이... 그래서 답은 k" 형태로 되어있다. 따라서 마지막 부분을 추출하면 model의 답안 $\hat{y}$ 을 얻을 수 있다.
이 과정을 *EXTRACT()*라고 하자. 즉, $\hat{y} \sim $ *EXTRACT(* $\hat{e}$ *)*이다. $\hat{y}$ 와 실제 답 y의 일치 여부를 통해 $\pi_\theta$ 를 update한다. 