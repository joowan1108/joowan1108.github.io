---
layout: single
title: "Linear Regression"
categories: ML
tag: [머신러닝]
author_profile: false
sidebar:
    nav: "counts"
toc: true
toc_sticky: true
toc_label: Table of Contents
use_math: true
---

# Supervised Learning: Linear Regression

## 1. Supervised Learning (지도 학습)

**지도 학습(Supervised Learning)**은 입력값 공간 $X$를 출력값 공간 $Y$로 매핑하는 함수 $h$를 학습하는 과정이다. 이때 함수 $h$를 **가설(Hypothesis)**이라고 부르는데, 우리가 만들고자 하는 모델에 해당한다.

지도 학습은 목표값(Target)의 형태에 따라 다음과 같이 나뉜다.
* **회귀 (Regression)**: 목표값이 연속적인 숫자일 경우 (e.g., 집값 예측)
* **분류 (Classification)**: 목표값이 이산적인 카테고리일 경우 (e.g., 스팸 메일 분류)

---

## 2. Linear Regression (선형 회귀)

**선형 회귀(Linear Regression)**는 데이터 점들 사이의 관계를 가장 잘 나타내는 하나의 **직선**을 찾는 통계 기법이다. 이 직선 모델을 통해 한 변수의 값을 가지고 다른 변수의 값을 예측하는 데 사용된다.

가설(모델) $h_\theta(x)$는 다음과 같이 정의된다.

$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + ... + \theta_d x_d = \sum_{i=0}^{d} \theta_i x_i = \theta^T x
$$

선형 회귀의 목표는 이 가설 함수 $h_\theta(x)$가 실제 데이터 값 $y$를 가장 정확하게 예측하도록 파라미터 $\theta$를 찾는 것이다.

---

## 3. 학습 과정

### 비용 함수 (Cost Function)
모든 지도 학습 기법은 모델의 예측값($h_\theta(x)$)과 실제 정답($y$)의 차이를 측정하는 기준이 필요하다. 이를 **비용(Cost)** 또는 **손실(Loss)**이라고 부른다. 선형 회귀에서는 예측값과 실제값의 차이(오차)를 이용해 비용을 정의하는데, 사용하는 비용 함수는 다음과 같다. (n은 데이터셋의 크기)

$$
J(\theta) = \frac{1}{2} \sum_{i=1}^{n} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

이 함수를 **최소 제곱법(Least Squares)** 비용 함수라고 부른다. 우리의 목표는 이 $J(\theta)$ 값을 최소화하는 파라미터 $\theta$를 찾는 것이다.

### 경사 하강법 (Gradient Descent)
비용 함수 $J(\theta)$를 최소화하기 위해 **경사 하강법(Gradient Descent)**을 사용한다. 경사 하강법은 비용 함수의 기울기(gradient)가 가장 가파르게 감소하는 방향으로 파라미터 $\theta$를 반복적으로 업데이트하는 방법이다.

각 파라미터 $\theta_j$에 대한 업데이트 규칙은 다음과 같다.

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$

여기서 $\alpha$는 **학습률(Learning Rate)**을 의미한다.

#### 비용 함수 미분
업데이트 규칙을 완성하기 위해 비용 함수 $J(\theta)$를 $\theta_j$에 대해 편미분해야 한다. (계산의 편의를 위해 데이터 1개에 대한 비용을 먼저 계산)

$$
\frac{\partial}{\partial \theta_j} \frac{1}{2} (h_\theta(x) - y)^2 = (h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_j} (h_\theta(x) - y)
$$

$h_\theta(x) = \sum_{k=0}^{d} \theta_k x_k$ 이므로, $\theta_j$에 대해 미분하면 $x_j$만 남는다.

$$
\frac{\partial}{\partial \theta_j} J(\theta) = (h_\theta(x) - y) \cdot x_j
$$

#### 최종 업데이트 규칙
위 미분 결과를 전체 데이터셋($i=1$부터 $n$까지)에 대한 합으로 확장하고, 최종 업데이트 규칙에 대입하면 다음과 같다. (아래는 단일 데이터 $i$에 대한 업데이트 규칙인 **확률적 경사 하강법(SGD)**의 예시다.)

$$
\theta_j := \theta_j - \alpha (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
$$

이 규칙의 의미는 간단하다. 예측값($h_\theta(x^{(i)})$)과 실제값($y^{(i)}$)의 **오차**가 클수록, 파라미터 $\theta$를 더 큰 폭으로 조정하여 모델이 정답에 가까워지도록 학습하는 것이다.


## 4. 확률적 해석: 왜 최소 제곱법을 사용할까?

**최소 제곱 비용 함수(Least squares cost function)**를 사용하는 것이 왜 합리적인 선택일까? 그 이유는 데이터의 오차(error)가 **정규분포(Gaussian)**를 따른다고 가정할 때, 최소 제곱법이 모델의 **가능도(Likelihood)**를 최대화하는 방법과 수학적으로 정확히 일치하기 때문이다.

### 모델과 오차의 가정

답 $y$와 입력 $x$의 관계를 다음과 같은 식으로 표현할 수 있다.

$$y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)}$$

여기서 $\epsilon^{(i)}$은 측정 과정에서 발생하는 **임의의 노이즈(random noise)**나 모델이 설명하지 못하는 효과들을 포함하는 **오차항(error term)**이다. 이 오차항이 평균이 0이고 분산이 $\sigma^2$인 정규분포를 따른다고 가정하자.

$$\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^2)$$

이 가정에 따라 오차항 $\epsilon^{(i)}$의 확률 밀도 함수(probability density function)는 다음과 같다.

$$p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right)$$

### 가능도(Likelihood) 함수

위의 관계식을 변형하면 $\epsilon^{(i)} = y^{(i)} - \theta^T x^{(i)}$ 이므로, 주어진 $x^{(i)}$와 파라미터 $\theta$에 대한 $y^{(i)}$의 조건부 확률 분포를 유도할 수 있다. 이는 오차의 분포를 $\theta^T x^{(i)}$만큼 평행이동한 것과 같다.

$$y^{(i)} \mid x^{(i)}; \theta \sim \mathcal{N}(\theta^T x^{(i)}, \sigma^2)$$

따라서 $y^{(i)}$의 조건부 확률 밀도 함수는 다음과 같다.

$$p(y^{(i)} \mid x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}\right)$$

학습 데이터셋 전체에 대한 가능도 함수 $L(\theta)$는 각 데이터 포인트의 확률의 곱으로 나타낼 수 있다 (각 데이터의 오차 $\epsilon^{(i)}$가 독립적이라고 가정).

$$L(\theta) = \prod_{i=1}^{n} p(y^{(i)} \mid x^{(i)}; \theta) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}\right)$$

### 최대 가능도 추정 (Maximum Likelihood Estimation)

우리의 목표는 이 가능도 함수 $L(\theta)$를 **최대화**하는 파라미터 $\theta$를 찾는 것이다. 계산의 편의를 위해 곱셈을 덧셈으로 바꾸는 로그(log)를 $L(\theta)$에 적용하여 **로그 가능도 함수(log-likelihood function)** $l(\theta)$를 만든다.

$$
\begin{align*}
l(\theta) &= \log(L(\theta)) \\
&= \log \left( \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}\right) \right) \\
&= \sum_{i=1}^{n} \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}\right) \right] \\
&= \sum_{i=1}^{n} \left[ \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2} \right] \\
&= n \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y^{(i)} - \theta^T x^{(i)})^2
\end{align*}
$$

### 결론: 최소 제곱법과의 연결

로그 가능도 $l(\theta)$를 최대화하려면, 위 식에서 $\theta$와 무관한 첫 번째 항을 제외하고, **음수 부호가 붙은 두 번째 항을 최소화**해야 한다. 즉, 다음 값을 최소화하는 것과 같다.

$$\sum_{i=1}^{n} (y^{(i)} - \theta^T x^{(i)})^2$$

이 식은 바로 **최소 제곱 비용 함수**와 정확히 일치한다.

결론적으로, **오차가 정규분포를 따른다는 합리적인 가정** 하에서 데이터의 예측 확률을 가장 높이는 파라미터 $\theta$를 찾는 **최대 가능도 추정법(Maximum Likelihood Estimation)**을 적용하면, 자연스럽게 **최소 제곱법(Least Squares)**이라는 결과에 도달하게 된다. 이것이 선형 회귀에서 최소 제곱법을 사용하는 것이 통계적으로 매우 타당한 이유다.